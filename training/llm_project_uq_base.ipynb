{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from transformers import MT5ForConditionalGeneration, AutoTokenizer\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from datasets import load_dataset\n",
    "from evaluate import load\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from rouge_score import rouge_scorer\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tqdm import tqdm \n",
    "from scipy.stats import kurtosis\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from scipy.stats import hmean\n",
    "from scipy.stats import gmean\n",
    "from evaluate import load\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt \n",
    "import evaluate\n",
    "import torch\n",
    "import time\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "\n",
    "nltk.download('wordnet')\n",
    "bertscore = load(\"bertscore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jhpark: verified that this is a correct way to use this \n",
    "def calculate_rouge(true_sentence, predicted_sentence):\n",
    "    # jhpark: rouge1/rouge2 (e.g. rouge1, rouge2): n-gram based scoring.\n",
    "    # jhpark: rougeL: Longest common subsequence based scoring.\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(true_sentence, predicted_sentence)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# jhpark: verified that this is a correct way to use this \n",
    "def calculate_bleu(true_tokens, predicted_tokens):\n",
    "    '''\n",
    "    * reference for smoothing: A Systematic Comparison of Smoothing Techniques for Sentence-Level BLEU, Boxing Chen and Collin Cherry (2014)\n",
    "    * method1: Smoothing method 1: Add *epsilon* counts to precision with 0 counts.\n",
    "    * https://www.nltk.org/_modules/nltk/translate/bleu_score.html for more details\n",
    "    '''\n",
    "    bleu_score = sentence_bleu(true_tokens, predicted_tokens, smoothing_function=SmoothingFunction().method1)\n",
    "    return bleu_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# english to X is only possible for T5\n",
    "from datasets import load_dataset, load_from_disk\n",
    "\n",
    "# ds_de_en = load_dataset(\"wmt/wmt14\", \"de-en\")\n",
    "# ds_fr_en = load_dataset(\"wmt/wmt15\", \"fr-en\")\n",
    "# ds_ro_en = load_dataset(\"wmt/wmt16\", \"ro-en\")\n",
    "\n",
    "# ds_de_en.save_to_disk(\"../wmt14_de_en\")\n",
    "# ds_fr_en.save_to_disk(\"../wmt15_fr_en\")\n",
    "# ds_ro_en.save_to_disk(\"../wmt16_ro_en\")\n",
    "\n",
    "ds_de_en = load_from_disk(\"../wmt14_de_en\")\n",
    "ds_fr_en = load_from_disk(\"../wmt15_fr_en\")\n",
    "ds_ro_en = load_from_disk(\"../wmt16_ro_en\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with base\n"
     ]
    }
   ],
   "source": [
    "# t5_tokenizer_small = AutoTokenizer.from_pretrained('t5-small')\n",
    "# t5_model_small = AutoModelForSeq2SeqLM.from_pretrained('t5-small')\n",
    "# print(\"Done with small\")\n",
    "\n",
    "t5_tokenizer_base = AutoTokenizer.from_pretrained('t5-base')\n",
    "t5_model_base = AutoModelForSeq2SeqLM.from_pretrained('t5-base')\n",
    "print(\"Done with base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_probs_from_logits(logits):\n",
    "    return F.softmax(logits, dim=-1)\n",
    "\n",
    "def generate_output_with_probs(model, tokenizer, example, top_k=5):\n",
    "    '''\n",
    "    - High Kurtosis: An indicator that the data has heavy tails or outliers.\n",
    "    - Low Kurtosis:  An indicator that the data has thin tails and lacks outliers.\n",
    "    '''\n",
    "    inputs = tokenizer.encode(example, return_tensors=\"pt\")\n",
    "    output_ids = model.generate(inputs, max_new_tokens=np.inf, return_dict_in_generate=True, output_scores=True, return_legacy_cache=False)\n",
    "    output_tokens = output_ids.sequences[0]\n",
    "    output_probs = []\n",
    "    output_kurtosis = []\n",
    "\n",
    "    # Retrieve probabilities for each token\n",
    "    for i in range(1, len(output_tokens) - 1):  # Skip the first token and the last token\n",
    "        # Fetch the logits from output_ids.scores, aligning with output_tokens[1:]\n",
    "        probs = calculate_probs_from_logits(output_ids.scores[i - 1])[0]  # i-1 to align with `scores` index\n",
    "        token_id = output_tokens[i]\n",
    "        token_prob = probs[token_id].item()\n",
    "\n",
    "        # Compute kurtosis on top probabilities\n",
    "        top_probs, _ = torch.topk(probs, 1000)\n",
    "        kurt = kurtosis(top_probs)\n",
    "\n",
    "        output_probs.append(token_prob)\n",
    "        output_kurtosis.append(kurt)\n",
    "    \n",
    "    decoded_output = tokenizer.decode(output_tokens, skip_special_tokens=True)\n",
    "    return decoded_output, np.array(output_probs), np.array(output_kurtosis)\n",
    "\n",
    "def visualization_1(token_probs, scaled_token_kurtosis):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "    ax1.plot(np.arange(0, len(token_probs)), token_probs, label='Token probability', c='red', marker='o')\n",
    "    ax1.plot(np.arange(0, len(token_probs)), scaled_token_kurtosis, label='Token kurtosis', c='blue', marker='^')\n",
    "    ax1.legend()\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(token_probs.reshape(-1, 1), scaled_token_kurtosis)\n",
    "    predictions = model.predict(token_probs.reshape(-1, 1))\n",
    "    r2 = r2_score(scaled_token_kurtosis, predictions)\n",
    "    ax2.scatter(token_probs, scaled_token_kurtosis, color='blue')\n",
    "    ax2.plot(token_probs, predictions, color='red', label=f'Regression Line (RÂ² = {r2:.2f})')\n",
    "    ax2.legend()\n",
    "    plt.show()\n",
    "\n",
    "def visualization_2(score_collector_en_X):   \n",
    "    # bleu\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n",
    "    ax1.scatter(score_collector_en_X['bleu'], score_collector_en_X['Confidence-G-T'])\n",
    "    ax2.scatter(score_collector_en_X['bleu'], score_collector_en_X['Confidence-A-T'])\n",
    "    ax3.scatter(score_collector_en_X['bleu'], score_collector_en_X['Confidence-A-K'])\n",
    "    ax1.set_ylabel('Confidence-G-T')\n",
    "    ax2.set_ylabel('Confidence-A-T')\n",
    "    ax3.set_ylabel('Confidence-A-K')\n",
    "    ax1.set_xlabel('bleu')\n",
    "    ax2.set_xlabel('bleu')\n",
    "    ax3.set_xlabel('bleu')\n",
    "    plt.plot()\n",
    "\n",
    "    # meteor\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n",
    "    ax1.scatter(score_collector_en_X['meteor'], score_collector_en_X['Confidence-G-T'])\n",
    "    ax2.scatter(score_collector_en_X['meteor'], score_collector_en_X['Confidence-A-T'])\n",
    "    ax3.scatter(score_collector_en_X['meteor'], score_collector_en_X['Confidence-A-K'])\n",
    "    ax1.set_ylabel('Confidence-G-T')\n",
    "    ax2.set_ylabel('Confidence-A-T')\n",
    "    ax3.set_ylabel('Confidence-A-K')\n",
    "    ax1.set_xlabel('meteor')\n",
    "    ax2.set_xlabel('meteor')\n",
    "    ax3.set_xlabel('meteor')\n",
    "    plt.plot()\n",
    "\n",
    "    # bert_f1s\n",
    "    bert_f1s = []\n",
    "    for i in range(samples):\n",
    "        bert_f1 = score_collector_en_X['bert'][i]['f1']\n",
    "        bert_f1s.append(bert_f1)\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n",
    "    ax1.scatter(bert_f1s, score_collector_en_X['Confidence-G-T'])\n",
    "    ax2.scatter(bert_f1s, score_collector_en_X['Confidence-A-T'])\n",
    "    ax3.scatter(bert_f1s, score_collector_en_X['Confidence-A-K'])\n",
    "    ax1.set_ylabel('Confidence-G-T')\n",
    "    ax2.set_ylabel('Confidence-A-T')\n",
    "    ax3.set_ylabel('Confidence-A-K')\n",
    "    ax1.set_xlabel('bert_f1s')\n",
    "    ax2.set_xlabel('bert_f1s')\n",
    "    ax3.set_xlabel('bert_f1s')\n",
    "    plt.plot()\n",
    "\n",
    "    # rouge_1s \n",
    "    rouge_1s = []\n",
    "    for i in range(samples):\n",
    "        rouge_1 = score_collector_en_X['rouge'][i]['rouge1'].fmeasure\n",
    "        rouge_1s.append(rouge_1)\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n",
    "    ax1.scatter(rouge_1s, score_collector_en_X['Confidence-G-T'])\n",
    "    ax2.scatter(rouge_1s, score_collector_en_X['Confidence-A-T'])\n",
    "    ax3.scatter(rouge_1s, score_collector_en_X['Confidence-A-K'])\n",
    "    ax1.set_ylabel('Confidence-G-T')\n",
    "    ax2.set_ylabel('Confidence-A-T')\n",
    "    ax3.set_ylabel('Confidence-A-K')\n",
    "    ax1.set_xlabel('rouge_1s')\n",
    "    ax2.set_xlabel('rouge_1s')\n",
    "    ax3.set_xlabel('rouge_1s')\n",
    "    plt.plot()\n",
    "\n",
    "    # rouge_2s\n",
    "    rouge_2s = []\n",
    "    for i in range(samples):\n",
    "        rouge_2 = score_collector_en_X['rouge'][i]['rouge2'].fmeasure\n",
    "        rouge_2s.append(rouge_2)\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n",
    "    ax1.scatter(rouge_2s, score_collector_en_X['Confidence-G-T'])\n",
    "    ax2.scatter(rouge_2s, score_collector_en_X['Confidence-A-T'])\n",
    "    ax3.scatter(rouge_2s, score_collector_en_X['Confidence-A-K'])\n",
    "    ax1.set_ylabel('Confidence-G-T')\n",
    "    ax2.set_ylabel('Confidence-A-T')\n",
    "    ax3.set_ylabel('Confidence-A-K')\n",
    "    ax1.set_xlabel('rouge_2s')\n",
    "    ax2.set_xlabel('rouge_2s')\n",
    "    ax3.set_xlabel('rouge_2s')\n",
    "    plt.plot()\n",
    "\n",
    "    # rouge_Ls\n",
    "    rouge_Ls = []\n",
    "    for i in range(samples):\n",
    "        rouge_L = score_collector_en_X['rouge'][i]['rougeL'].fmeasure\n",
    "        rouge_Ls.append(rouge_L)\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 3))\n",
    "    ax1.scatter(rouge_Ls, score_collector_en_X['Confidence-G-T'])\n",
    "    ax2.scatter(rouge_Ls, score_collector_en_X['Confidence-A-T'])\n",
    "    ax3.scatter(rouge_Ls, score_collector_en_X['Confidence-A-K'])\n",
    "    ax1.set_ylabel('Confidence-G-T')\n",
    "    ax2.set_ylabel('Confidence-A-T')\n",
    "    ax3.set_ylabel('Confidence-A-K')\n",
    "    ax1.set_xlabel('rouge_Ls')\n",
    "    ax2.set_xlabel('rouge_Ls')\n",
    "    ax3.set_xlabel('rouge_Ls')\n",
    "    plt.plot()\n",
    "\n",
    "score_collector_en_de = {'bleu':[], 'meteor':[], 'rouge':[], 'bert':[], 'Confidence-G-T':[], 'Confidence-A-T':[], 'Confidence-A-K':[]}\n",
    "score_collector_en_fr = {'bleu':[], 'meteor':[], 'rouge':[], 'bert':[], 'Confidence-G-T':[], 'Confidence-A-T':[], 'Confidence-A-K':[]}\n",
    "score_collector_en_ro = {'bleu':[], 'meteor':[], 'rouge':[], 'bert':[], 'Confidence-G-T':[], 'Confidence-A-T':[], 'Confidence-A-K':[]}\n",
    "samples = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Generate translations (English to German)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]c:\\Users\\user\\miniconda3\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:403: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "100%|ââââââââââ| 100/100 [02:18<00:00,  1.39s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(samples)):\n",
    "    # print(f\"--------------- Sample {i} ---------------\")\n",
    "    input_text = f\"translate English to German: {ds_de_en['train'][i]['translation']['en']}\"\n",
    "    true_translation = ds_de_en['train'][i]['translation']['de']\n",
    "    predicted_translation, token_probs, token_kurtosis = generate_output_with_probs(model=t5_model_base, tokenizer=t5_tokenizer_base, example=input_text)\n",
    "\n",
    "    # tokenize \n",
    "    true_tokens = t5_tokenizer_base.tokenize(true_translation)\n",
    "    predicted_tokens = t5_tokenizer_base.tokenize(predicted_translation)\n",
    "\n",
    "    # uncertainty quantification using kurtosis \n",
    "    token_kurtosis = token_kurtosis.reshape(-1, 1)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_token_kurtosis = scaler.fit_transform(token_kurtosis).flatten() \n",
    "\n",
    "    # already available metrics\n",
    "    bleu   = calculate_bleu([true_tokens], predicted_tokens)                                            # 1. BLEU\n",
    "    meteor = meteor_score([true_tokens], predicted_tokens)                                              # 2. METEOR (# jhpark: verified that this is a correct way to use this.)\n",
    "    rouge  = calculate_rouge(true_translation, predicted_translation)                                   # 3. ROUGE\n",
    "    bert = bertscore.compute(predictions=[predicted_translation], references=[true_translation], model_type='distilbert-base-multilingual-cased') # 4. BERTscore.\n",
    "    \n",
    "    score_collector_en_de['bleu'].append(bleu)\n",
    "    score_collector_en_de['meteor'].append(meteor)\n",
    "    score_collector_en_de['rouge'].append(rouge)\n",
    "    score_collector_en_de['bert'].append(bert)\n",
    "\n",
    "    score_collector_en_de['Confidence-G-T'].append(gmean(token_probs))\n",
    "    score_collector_en_de['Confidence-A-T'].append(np.average(token_probs))\n",
    "    score_collector_en_de['Confidence-A-K'].append(np.average(scaled_token_kurtosis))\n",
    "\n",
    "    # print results\n",
    "    # print(\"[Sentences]\")\n",
    "    # print(\"   Input:\", input_text)\n",
    "    # print(\"   True Translation:\", true_translation)\n",
    "    # print(\"   Predicted Translation:\", predicted_translation)\n",
    "\n",
    "    # print(\"\\n[Scores]\")\n",
    "    # print(\"   BLEU score:\", bleu) \n",
    "    # print(\"   METEOR score:\", meteor) \n",
    "    # print(\"   ROUGE score:\", rouge) \n",
    "    # print(\"   BERT score:\", bert) \n",
    "\n",
    "    # print(\"\\n[Uncertainty Quantification]\")\n",
    "    # print(f\"   [New UQ] Confidence-G-T (Geometric mean of token probability): {np.round(gmean(token_probs), 3)}\") \n",
    "    # print(f\"   [New UQ] Confidence-A-T (Arithmetic mean of token probability): {np.round(np.average(token_probs), 3)}\") \n",
    "    # print(f\"   [New UQ] Confidence-A-K (Arithmetic mean of kurtosis): {np.round(np.average(scaled_token_kurtosis), 3)}\") \n",
    "    \n",
    "    # visualization_1(token_probs, scaled_token_kurtosis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization_2(score_collector_en_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Generate translations (English to French)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 100/100 [02:41<00:00,  1.62s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(samples)):\n",
    "    # print(f\"--------------- Sample {i} ---------------\")\n",
    "    input_text = f\"translate English to French: {ds_fr_en['train'][i]['translation']['en']}\"\n",
    "    true_translation = ds_fr_en['train'][i]['translation']['fr']\n",
    "    predicted_translation, token_probs, token_kurtosis = generate_output_with_probs(model=t5_model_base, tokenizer=t5_tokenizer_base, example=input_text)\n",
    "\n",
    "    # tokenize \n",
    "    true_tokens = t5_tokenizer_base.tokenize(true_translation)\n",
    "    predicted_tokens = t5_tokenizer_base.tokenize(predicted_translation)\n",
    "\n",
    "    # uncertainty quantification using kurtosis \n",
    "    token_kurtosis = token_kurtosis.reshape(-1, 1)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_token_kurtosis = scaler.fit_transform(token_kurtosis).flatten() \n",
    "\n",
    "    # already available metrics\n",
    "    bleu   = calculate_bleu([true_tokens], predicted_tokens)                                            # 1. BLEU\n",
    "    meteor = meteor_score([true_tokens], predicted_tokens)                                              # 2. METEOR (# jhpark: verified that this is a correct way to use this.)\n",
    "    rouge  = calculate_rouge(true_translation, predicted_translation)                                   # 3. ROUGE\n",
    "    bert = bertscore.compute(predictions=[predicted_translation], references=[true_translation], model_type='distilbert-base-multilingual-cased') # 4. BERTscore.\n",
    "\n",
    "    score_collector_en_fr['bleu'].append(bleu)\n",
    "    score_collector_en_fr['meteor'].append(meteor)\n",
    "    score_collector_en_fr['rouge'].append(rouge)\n",
    "    score_collector_en_fr['bert'].append(bert)\n",
    "\n",
    "    score_collector_en_fr['Confidence-G-T'].append(gmean(token_probs))\n",
    "    score_collector_en_fr['Confidence-A-T'].append(np.average(token_probs))\n",
    "    score_collector_en_fr['Confidence-A-K'].append(np.average(scaled_token_kurtosis))\n",
    "\n",
    "    # print results\n",
    "    # print(\"[Sentences]\")\n",
    "    # print(\"   Input:\", input_text)\n",
    "    # print(\"   True Translation:\", true_translation)\n",
    "    # print(\"   Predicted Translation:\", predicted_translation)\n",
    "\n",
    "    # print(\"\\n[Scores]\")\n",
    "    # print(\"   BLEU score:\", bleu) \n",
    "    # print(\"   METEOR score:\", meteor)\n",
    "    # print(\"   ROUGE score:\", rouge)\n",
    "    # print(\"   BERT score:\", bert) \n",
    "\n",
    "    # print(\"\\n[Uncertainty Quantification]\")\n",
    "    # print(f\"   [New UQ] Confidence-G-T (Geometric mean of token probability): {np.round(gmean(token_probs), 3)}\") \n",
    "    # print(f\"   [New UQ] Confidence-A-T (Arithmetic mean of token probability): {np.round(np.average(token_probs), 3)}\") \n",
    "    # print(f\"   [New UQ] Confidence-A-K (Arithmetic mean of kurtosis): {np.round(np.average(scaled_token_kurtosis), 3)}\") \n",
    "\n",
    "    # visualization_1(token_probs, scaled_token_kurtosis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization_2(score_collector_en_fr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate translations (English to Romanian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|ââââââââââ| 100/100 [01:07<00:00,  1.48it/s]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(samples)):\n",
    "    # print(f\"--------------- Sample {i} ---------------\")\n",
    "    input_text = f\"translate English to Romanian: {ds_ro_en['train'][i]['translation']['en']}\"\n",
    "    true_translation = ds_ro_en['train'][i]['translation']['ro']\n",
    "    predicted_translation, token_probs, token_kurtosis = generate_output_with_probs(model=t5_model_base, tokenizer=t5_tokenizer_base, example=input_text)\n",
    "\n",
    "    # tokenize \n",
    "    true_tokens = t5_tokenizer_base.tokenize(true_translation)\n",
    "    predicted_tokens = t5_tokenizer_base.tokenize(predicted_translation)\n",
    "\n",
    "    # uncertainty quantification using kurtosis \n",
    "    token_kurtosis = token_kurtosis.reshape(-1, 1)\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_token_kurtosis = scaler.fit_transform(token_kurtosis).flatten() \n",
    "\n",
    "    # already available metrics\n",
    "    bleu   = calculate_bleu([true_tokens], predicted_tokens)                                            # 1. BLEU\n",
    "    meteor = meteor_score([true_tokens], predicted_tokens)                                              # 2. METEOR (# jhpark: verified that this is a correct way to use this.)\n",
    "    rouge  = calculate_rouge(true_translation, predicted_translation)                                   # 3. ROUGE\n",
    "    bert = bertscore.compute(predictions=[predicted_translation], references=[true_translation], model_type='distilbert-base-multilingual-cased') # 4. BERTscore.\n",
    "\n",
    "    score_collector_en_ro['bleu'].append(bleu)\n",
    "    score_collector_en_ro['meteor'].append(meteor)\n",
    "    score_collector_en_ro['rouge'].append(rouge)\n",
    "    score_collector_en_ro['bert'].append(bert)\n",
    "\n",
    "    score_collector_en_ro['Confidence-G-T'].append(gmean(token_probs))\n",
    "    score_collector_en_ro['Confidence-A-T'].append(np.average(token_probs))\n",
    "    score_collector_en_ro['Confidence-A-K'].append(np.average(scaled_token_kurtosis))\n",
    "\n",
    "    # print results\n",
    "    # print(\"[Sentences]\")\n",
    "    # print(\"   Input:\", input_text)\n",
    "    # print(\"   True Translation:\", true_translation)\n",
    "    # print(\"   Predicted Translation:\", predicted_translation)\n",
    "\n",
    "    # print(\"\\n[Scores]\")\n",
    "    # print(\"   BLEU score:\", bleu); \n",
    "    # print(\"   METEOR score:\", meteor) \n",
    "    # print(\"   ROUGE score:\", rouge) \n",
    "    # print(\"   BERT score:\", bert) \n",
    "\n",
    "    # print(\"\\n[Uncertainty Quantification]\")\n",
    "    # print(f\"   [New UQ] Confidence-G-T (Geometric mean of token probability): {np.round(gmean(token_probs), 3)}\") \n",
    "    # print(f\"   [New UQ] Confidence-A-T (Arithmetic mean of token probability): {np.round(np.average(token_probs), 3)}\") \n",
    "    # print(f\"   [New UQ] Confidence-A-K (Arithmetic mean of kurtosis): {np.round(np.average(scaled_token_kurtosis), 3)}\") \n",
    "\n",
    "    # visualization_1(token_probs, scaled_token_kurtosis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization_2(score_collector_en_ro)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dictionary to a binary file\n",
    "with open(\"score_collector_en_de_base.pkl\", \"wb\") as file:\n",
    "    pickle.dump(score_collector_en_de, file)\n",
    "\n",
    "with open(\"score_collector_en_fr_base.pkl\", \"wb\") as file:\n",
    "    pickle.dump(score_collector_en_fr, file)\n",
    "\n",
    "with open(\"score_collector_en_ro_base.pkl\", \"wb\") as file:\n",
    "    pickle.dump(score_collector_en_ro, file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following metrics cannot be used because we only have one-to-one translations.\n",
    "\n",
    "1. CIDEr: Consensus-based Image Description Evaluation\n",
    "2. SPICE: Semantic Propositional Image Caption Evaluation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
